{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqAi+YO46SF6z5/2Om5KAA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swguo/pySpark/blob/main/pySpark_%E5%AE%89%E8%A3%9D%E8%88%87%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfDG65SYz9xJ",
        "outputId": "9220cc17-bd59-41c0-a80e-526223c9da52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxtst6\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic fonts-ipafont-mincho\n",
            "  fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 30.8 MB of archives.\n",
            "After this operation, 104 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre-headless amd64 8u432-ga~us1-0ubuntu2~22.04 [30.8 MB]\n",
            "Fetched 30.8 MB in 1s (33.4 MB/s)\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install openjdk-8-jre-headless\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "XksjxjS00E3m",
        "outputId": "252775db-6efc-4ad3-da37-0fcc899115a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fdde9f11ea0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cf626f54d0bd:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 建立 RDD"
      ],
      "metadata": {
        "id": "axxcbCMN15wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([\"Hello Spark\"])\n",
        "counts = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .collect()\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7vQxHw_0E_3",
        "outputId": "17816b22-02c0-4ca5-8a76-22a4aeebd5cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 1), ('Spark', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD (Resilient Distributed Dataset) 是 PySpark 的基本資料結構，它是一個不可變的、可分割的、分散式的資料集合。您可以使用 parallelize() 方法從 Python 列表建立 RDD"
      ],
      "metadata": {
        "id": "o1FWYbfb0j4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2Vp0Pi30kPr",
        "outputId": "9ddbea9d-429a-4eb4-cc15-53367f3ce8ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[7] at readRDDFromFile at PythonRDD.scala:289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = rdd.collect()\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlw9AEYt1IoC",
        "outputId": "c29f7f9d-b4c6-411b-c5e7-e246af6ef361"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 執行轉換和動作\n",
        "\n",
        "PySpark 提供了許多轉換和動作來操作 RDD。轉換會建立一個新的 RDD，而動作會觸發計算並返回結果。以下是一些常見的轉換和動作：\n",
        "\n",
        "* map(): 將函數應用於 RDD 中的每個元素。\n",
        "* filter(): 根據條件過濾 RDD 中的元素。\n",
        "* reduce(): 將函數應用於 RDD 中的元素，並將結果聚合為單一值。\n",
        "* collect(): 將 RDD 中的所有元素收集到驅動程式節點。"
      ],
      "metadata": {
        "id": "HweLaGUK1t57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squared_rdd = rdd.map(lambda x: x * x)\n",
        "print(squared_rdd.collect())\n",
        "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "print(filtered_rdd.collect())\n",
        "sum_rdd = rdd.reduce(lambda x, y: x + y)\n",
        "print(sum_rdd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAXHSHQs0weF",
        "outputId": "26407b7e-d30c-4259-b547-1c78e8216168"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 9, 16, 25]\n",
            "[2, 4]\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 建立 DataFrame\n",
        "\n",
        "DataFrame 是一種以表格形式組織的資料結構，它類似於 Pandas DataFrame。您可以使用 createDataFrame() 方法從 RDD 或其他資料源建立 DataFrame："
      ],
      "metadata": {
        "id": "jYi1THoG1oYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])"
      ],
      "metadata": {
        "id": "oiNlj2EK0yfp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8TgHIFs0zfB",
        "outputId": "9940088b-f989-4479-af50-91a1bad6f449"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice| 25|\n",
            "|    Bob| 30|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 執行 SQL 查詢"
      ],
      "metadata": {
        "id": "M4cGmJIr1-gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"people\")\n",
        "result = spark.sql(\"SELECT * FROM people WHERE age > 30\")"
      ],
      "metadata": {
        "id": "jV2SHvtM2ARs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OC3c43H2H-D",
        "outputId": "0c354973-7a4a-4b0f-9da9-2505773c019e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 儲存資料"
      ],
      "metadata": {
        "id": "lSvmlhE12LJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format(\"csv\").save(\"people.csv\")"
      ],
      "metadata": {
        "id": "pci-kFlY2Luv"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}